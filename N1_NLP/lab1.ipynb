{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK corpus 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import _____\n",
    "from nltk.corpus import gutenberg\n",
    "print(gutenberg.fileids())\n",
    "# nltk.corpus 중 gutenberg에 있는 파일들을 확인합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### text 불러오기 (.raw, .words, .sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 중 \"burgress-busterbrwon.txt\"의 raw을 10개 불러오겠습니다.\n",
    "raw = gutenberg.raw(\"burgess-busterbrown.txt\")\n",
    "print(raw[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 문서의 word를 10개 불러옵니다.\n",
    "words = gutenberg.words(\"burgess-busterbrown.txt\")\n",
    "print(words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 문서의 sentence를 10개 불러옵니다.\n",
    "sents = gutenberg.sents(\"burgess-busterbrown.txt\")\n",
    "print(sents[0:10])\n",
    "\n",
    "# 이 세가지 버전의 차이점을 확인하셨나요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이렇게 gutenberg의 text중 하나를 불러왔는데, \n",
    "### 원하는 텍스트를 하나 골라서 원하는 버전으로 불러와 확인해봅시다.\n",
    "### your code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 활용1 : Concordance, Similar, Dispersion plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk에서 위와 다른 방법으로 text를 불러올 수 있습니다.\n",
    "# 이렇게 text를 9개 불러올 수 있습니다. \n",
    "# 이 창을 실행하면 각 텍스트에 대한 설명을 확인할 수 있습니다.\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 살펴볼 텍스트는 대통령 연설 코퍼스입니다. \n",
    "# text4: Inaugural Address Corpus\n",
    "text4.concordance(\"terror\") \n",
    "# text4에서 terror가 사용된 텍스트의 문장들을 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.similar(\"terror\") # 해당 단어가 사용된 '환경'이 유사한 단어들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text1: Moby Dick by Herman Melville 1851\n",
    "text1.similar(\"terror\") \n",
    "# text4의 terror와 유사한 환경에서 사용되는 단어들과 text1에서의 terror와 유사한 환경에서 사용되는 단어들은 다르다는 것을 확인할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\",\n",
    "                       \"freedom\", \"duties\", \"America\"]) # 시간순 사용빈도\n",
    "# text4에서 시간이 지남에 따라서 해당 단어가 사용된 빈도를 표로 확인할 수 있습니다. \n",
    "# 표가 보이지 않으면 두 번 실행해보세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 활용2 : Tokenize, Pos tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning \n",
    "            Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 문장을 단어 단위로 쪼갤 수 있습니다.\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만든 토큰들이 어떤 품사를 가지는 지 확인할 수 있습니다.\n",
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK 활용3 : nltk.Text() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞에서 load한 gutenberg에서 austen-emma 텍스트로 앞에서 적용한 nltk의 기능들을 다시 살펴보겠습니다.\n",
    "emma_txt = gutenberg.words(\"austen-emma.txt\")\n",
    "print(emma_txt[0:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emma_nltktext = nltk.Text(emma_txt)\n",
    "print(len(emma_nltktext.tokens)) # returns number of tokens (document length)\n",
    "print(len(emma_txt))  # check number of tokens\n",
    "\n",
    "# 단어 별로 나눈 토큰들 중 중복을 제거한 토큰의 개수입니다.\n",
    "print(len(set(emma_nltktext.tokens)))  # returns number of unique tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 토큰들이 몇 개 사용되었는지 볼 수 있습니다. \n",
    "emma_nltktext.vocab()  # returns frequency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 중 top30이 무엇이고, 몇 개씩 사용되었는지를 표로 볼 수 있습니다.\n",
    "emma_nltktext.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Emma'가 사용된 부분들 확인\n",
    "emma_nltktext.concordance('Emma', lines=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emma는 주인공이라 내내 사용되는데, Frank와 Jane은 책 초반이 지나야 등장하는 인물이네요!\n",
    "emma_nltktext.dispersion_plot(['Emma', 'Frank', 'Jane'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식을 알아보기위해서 wordlist를 하나 불러옵니다. \n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]\n",
    "print(wordlist[0:15]) # 불러온 wordlist 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlist에서 ed로 끝나는 단어들\n",
    "print([w for w in wordlist if re.search('ed$', w)][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)])\n",
    "\n",
    "# 비교\n",
    "# print([w for w in wordlist if re.search('^.[mno][jlk][def]$', w)])\n",
    "# print([w for w in wordlist if re.search('^[ghi].[jlk][def]$', w)])\n",
    "# print([w for w in wordlist if re.search('^[ghi][mno].[def]$', w)])\n",
    "# print([w for w in wordlist if re.search('^[ghi][mno][jlk].$', w)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re.findall()\n",
    "\n",
    "(\"find all\") method finds all (non-overlapping) matches of the given regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *, +의 차이\n",
    "\n",
    "source1 = \"ct cat caat caaat caaaat caaaaat caaaaaat\"\n",
    "m1 = re.findall(\"ca*t\", source1)\n",
    "m2 = re.findall(\"ca+t\", source1)\n",
    "print(\"m1 : \", m1)\n",
    "print(\"m2 : \", m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {}의 기능\n",
    "\n",
    "source1 = \"ct cat caat caaat caaaat\"\n",
    "m3 = re.findall(\"ca{2}t\", source1)\n",
    "m4 = re.findall(\"ca{2,5}t\", source1)\n",
    "m5 = re.findall(\"ca{0,}t\", source1)\n",
    "m6 = re.findall(\"ca{0,1}t\", source1)\n",
    "m7 = re.findall(\"ca{,3}t\", source1)\n",
    "print(\"m3 : \", m3)\n",
    "print(\"m4 : \", m4)\n",
    "print(\"m5 : \", m5)\n",
    "print(\"m6 : \", m6)\n",
    "print(\"m7 : \", m7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?의 기능\n",
    "\n",
    "source2 = \"ac abc\"\n",
    "m8 = re.findall(\"ab?c\", source2)\n",
    "print(\"m8 : \", m8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^의 기능\n",
    "\n",
    "m9 = re.findall(\"^Life\", \"Life is too short\")\n",
    "m10 = re.findall(\"^is\", \"Life is too short\")\n",
    "print(\"m9 결과 : \", m9)\n",
    "print(\"m10 결과 : \", m10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $의 기능\n",
    "\n",
    "m11 = re.findall(\"short$\", \"Life is too short\")\n",
    "m12 = re.findall(\"short$\", \"Life is too short. So what?\")\n",
    " \n",
    "print(\"m11 결과 : \", m11)\n",
    "print(\"m12 결과 : \", m12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 세 결과를 보면서, 해당 정규표현식을 이해해봅시다.\n",
    "print(re.findall('[aeiou]', 'supercalifragilisticexpialidocious'))\n",
    "print(re.findall(\"app\\w*\",\"application orange apple banana\"))\n",
    "print(re.findall('^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 위에서 불러온 wordlist를 이용해서 정규표현식을 만들고 원하는 결과들이 나오는지 확인해봅시다.\n",
    "### www.nltk.org/book/ch03.html에서 3.4와 3.5를 참고하세요.\n",
    "\n",
    "### your code\n",
    "### print([w for w in wordlist if re.search('________________', w)])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 원하는 sentence와 정규표현식을 지정해서, 원하는 결과들이 나오는지 확인해봅시다.\n",
    "### www.nltk.org/book/ch03.html에서 3.4와 3.5를 참고하세요.\n",
    "\n",
    "### your code\n",
    "### print(re.findall(\"_______________\", sentence))\n",
    "\n",
    "\n",
    "sentence = \" \"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import *\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_dict(gram_list):\n",
    "    # Returns a dict containing counts of each n-gram in `gram_list`.\n",
    "    d = defaultdict(lambda: 0)\n",
    "    for gram in gram_list:\n",
    "        d[gram] += 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = gutenberg.raw(\"austen-emma.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap every sentence with ^(<s>) and $(</s>)\n",
    "corpus = ''.join('^ ' + s + ' $' for s in sent_tokenize(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and remove punctuation\n",
    "unigrams = [re.sub('[_~\\*]', '', word) for word in word_tokenize(corpus) if re.search(r'[\\w\\^\\$]+', word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude ($, ^) pair from bigrams\n",
    "bigrams = [pair for pair in zip(unigrams[:-1], unigrams[1:]) if pair != ('$', '^')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_freq = freq_dict(unigrams)\n",
    "bigram_freq = freq_dict(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = '^ I think I will get the best score in this class $'\n",
    "target = target.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLE bigram probability of the sentence.\n",
    "$$P(w_{i}|w_{i-1}) = \\frac{count(w_{i-1}, w_{i})}{count(w_{i-1})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MLE_prob_sent(sent, unigram_freq, bigram_freq):\n",
    "    # Returns MLE probability of the given bigram, \n",
    "    # using add-1 (Laplace) smoothing  \n",
    "    source = zip(sent[:-1], sent[1:])\n",
    "    result = 1\n",
    "    for bigram in source:\n",
    "        MLE_prob = (bigram_freq[bigram] + 1) / (unigram_freq[bigram[0]] + len(unigram_freq))\n",
    "        print(bigram, ' ', '{:.8f}'.format(MLE_prob))\n",
    "        result *= MLE_prob\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MLE bigram probability of this sentence: ',\n",
    "      MLE_prob_sent(target, unigram_freq, bigram_freq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
